# Environment variables for Adaptive Speculative Decoding

# HuggingFace Token (required for Llama models)
HUGGINGFACE_TOKEN=your_hf_token_here

# Weights & Biases (optional)
WANDB_API_KEY=your_wandb_key_here
WANDB_PROJECT=adaptive-speculative-decoding

# CUDA Configuration
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# NCCL Configuration (for multi-GPU)
NCCL_P2P_DISABLE=1
NCCL_IB_DISABLE=1

# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
LOG_LEVEL=info

# Pipeline Configuration
LAMBDA_VALUE=1.0
ENABLE_CACHING=true
MAX_CONCURRENT_REQUESTS=100

# Model Configuration
QUANTIZATION_ENABLED=true
GPU_MEMORY_UTILIZATION=0.8

# Development
PYTHONPATH=/app/src
PYTHONHASHSEED=0