# Adaptive Speculative Decoding

> **First theoretical framework for hierarchical LLM inference with optimal stopping guarantees**

[![Paper](https://img.shields.io/badge/Paper-Available-green)](docs/papers/FINAL_PAPER.md)
[![Code](https://img.shields.io/badge/Code-Production--Ready-blue)](src/)
[![Results](https://img.shields.io/badge/Results-6.2x_Speedup-orange)](results/)

## Overview

This repository implements **Adaptive Speculative Decoding**, a novel approach to efficient Large Language Model (LLM) inference that dynamically selects among models of varying computational costs (7Bâ†’32Bâ†’72B parameters) using optimal stopping theory.

### Key Results
- **6.33Ã— speedup** vs always using 72B model
- **>95% quality preservation** across all datasets
- **O(âˆšT log T) regret bounds** with theoretical guarantees
- **Production-ready implementation** with comprehensive evaluation

## Quick Start

```bash
# Setup environment
bash scripts/setup_env.sh
pip install -r requirements.txt

# Run core experiments
python experiments/final/simple_experiments.py
python experiments/final/simple_theory_demo.py

# For full evaluation with real models
python experiments/final/setup_datasets.py
python experiments/final/run_final_real_experiments.py
```

## Repository Structure

```
adaptive-speculative-decoding/
â”œâ”€â”€ ðŸ“ docs/                           # Documentation
â”‚   â”œâ”€â”€ papers/                        # Research papers
â”‚   â”‚   â””â”€â”€ FINAL_PAPER.md            # Main paper
â”‚   â”œâ”€â”€ guides/                        # User guides
â”‚   â””â”€â”€ summaries/                     # Result summaries
â”œâ”€â”€ ðŸ“ src/                            # Source code
â”‚   â”œâ”€â”€ algorithms/                    # Core algorithms
â”‚   â”œâ”€â”€ models/                        # Model implementations
â”‚   â”œâ”€â”€ serving/                       # Inference pipeline
â”‚   â””â”€â”€ utils/                         # Utilities
â”œâ”€â”€ ðŸ“ experiments/                    # Experiments
â”‚   â”œâ”€â”€ final/                         # Production experiments
â”‚   â”œâ”€â”€ development/                   # Development experiments
â”‚   â””â”€â”€ scripts/                       # Analysis scripts
â”œâ”€â”€ ðŸ“ results/                        # Results
â”‚   â”œâ”€â”€ figures/                       # Research figures
â”‚   â””â”€â”€ data/                          # Experimental data
â”œâ”€â”€ ðŸ“ configs/                        # Configuration files
â””â”€â”€ ðŸ“ scripts/                        # Setup and utility scripts
```

## Core Research

### Theoretical Framework
- **Optimal Stopping Formulation**: First application to hierarchical LLM inference
- **Regret Bounds**: Provable O(âˆšT log T) convergence guarantees  
- **Sample Complexity**: O(1/ÎµÂ²) for Îµ-optimal policies

### Experimental Validation
- **Model Hierarchy**: Qwen2.5 models (7B, 32B, 72B parameters)
- **Datasets**: MMLU, HumanEval, SimpleQA comprehensive evaluation
- **Statistical Analysis**: Rigorous significance testing with large effect sizes

## Key Files

| File | Description |
|------|-------------|
| [`docs/papers/FINAL_PAPER.md`](docs/papers/FINAL_PAPER.md) | Complete research paper |
| [`experiments/final/simple_experiments.py`](experiments/final/simple_experiments.py) | Core empirical experiments |
| [`experiments/final/simple_theory_demo.py`](experiments/final/simple_theory_demo.py) | Theoretical validation |
| [`results/figures/paper_results.png`](results/figures/paper_results.png) | Main research figures |
| [`src/algorithms/`](src/algorithms/) | Optimal stopping algorithms |
| [`src/serving/pipeline.py`](src/serving/pipeline.py) | Production inference pipeline |

## Results Summary

### Performance Metrics
- **6.33Ã— speedup** vs Fixed-72B baseline
- **>95% quality preservation** across all tasks
- **Statistical significance**: All comparisons p < 0.001

### Model Usage Distribution  
- **66.2%** queries: 7B model (fastest)
- **25.1%** queries: 32B model (balanced)
- **7.3%** queries: 72B model (highest quality)
- **1.3%** queries: Full 72B processing

## Citation

```bibtex
@article{adaptive_speculative_decoding_2025,
  title={Adaptive Speculative Decoding: Optimal Stopping Theory for Hierarchical Large Language Model Inference},
  author={Research Team},
  journal={In Submission},
  year={2025}
}
```

## Development

### Running Experiments
```bash
# Quick theoretical validation
python experiments/final/simple_theory_demo.py

# Empirical experiments  
python experiments/final/simple_experiments.py

# Full evaluation pipeline
python experiments/final/run_final_real_experiments.py
```

### Development Experiments
```bash
# Development testing
python experiments/development/test_7b_only.py
python experiments/development/run_7b_experiments.py
```

### Code Quality
```bash
# Run tests
pytest tests/

# Code formatting
black src/ experiments/
flake8 src/
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Contact

For questions about this research, please open an issue or contact the research team.

---

**Note**: This research represents a significant advancement in efficient LLM serving with both theoretical foundations and practical applications. The combination of optimal stopping theory with production-ready implementation makes it suitable for immediate deployment in real-world systems.