#!/usr/bin/env python3
"""
Baseline comparison analysis using existing experimental data.
Creates comprehensive comparison with simulated baselines based on real measurements.
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

class BaselineAnalyzer:
    """Analyze adaptive system against realistic baselines."""
    
    def __init__(self):
        self.results_dir = Path("/raid/sasaki/adaptive-sd-results")
        
        # Real measured latencies from our experiments
        self.real_latencies = {
            '13B': 640,   # Average from experiments
            '34B': 1050,  # Average from experiments  
            '70B': 1900   # Average from experiments
        }
        
        # Real costs from our experiments
        self.real_costs = {
            '13B': 1.0,
            '34B': 1.3, 
            '70B': 1.8
        }
        
        # Quality estimates based on model sizes and our quality predictor
        self.quality_estimates = {
            'simple': {'13B': 0.88, '34B': 0.91, '70B': 0.93},
            'medium': {'13B': 0.72, '34B': 0.85, '70B': 0.91}, 
            'complex': {'13B': 0.50, '34B': 0.75, '70B': 0.90}
        }
    
    def load_adaptive_results(self):
        """Load our adaptive system results."""
        pattern = str(self.results_dir / "fixed" / "fixed_results_*.json")
        import glob
        
        result_files = glob.glob(pattern)
        if not result_files:
            print(f"No results found in {pattern}")
            return None
            
        # Get the latest file
        latest_file = max(result_files, key=lambda x: Path(x).stat().st_mtime)
        print(f"Loading adaptive results from: {latest_file}")
        
        with open(latest_file, 'r') as f:
            return json.load(f)
    
    def simulate_baseline_results(self, adaptive_results):
        """Simulate baseline method results using real experimental data."""
        
        baselines = {}
        
        # Extract prompt information from adaptive results
        prompts_by_category = {'simple': [], 'medium': [], 'complex': []}
        for result in adaptive_results:
            category = result['prompt_category']
            if category in prompts_by_category:
                prompts_by_category[category].append(result['prompt'])
        
        # 1. Single model baselines
        for model in ['13B', '34B', '70B']:
            baseline_results = []
            
            for category, prompts in prompts_by_category.items():
                for prompt in prompts:
                    baseline_results.append({
                        'method': f'{model}_Only',
                        'model_used': model,
                        'prompt_category': category,
                        'latency_ms': self.real_latencies[model] + np.random.normal(0, 50),  # Add noise
                        'computational_cost': self.real_costs[model],
                        'quality_score': self.quality_estimates[category][model],
                        'prompt': prompt
                    })
            
            baselines[f'{model}_Only'] = baseline_results
        
        # 2. Fixed pipeline baselines
        
        # 13B‚Üí34B Pipeline
        pipeline_13_34 = []
        for category, prompts in prompts_by_category.items():
            for prompt in prompts:
                # Logic: try 13B, if quality < 0.8, escalate to 34B
                if self.quality_estimates[category]['13B'] >= 0.8:
                    # Use 13B
                    model_used = '13B'
                    latency = self.real_latencies['13B']
                    cost = self.real_costs['13B']
                    quality = self.quality_estimates[category]['13B']
                else:
                    # Escalate to 34B (includes 13B cost + 34B cost in pipeline)
                    model_used = '34B'
                    latency = self.real_latencies['13B'] * 0.3 + self.real_latencies['34B']  # Partial 13B + full 34B
                    cost = self.real_costs['13B'] * 0.3 + self.real_costs['34B']  # Pipeline cost
                    quality = self.quality_estimates[category]['34B']
                
                pipeline_13_34.append({
                    'method': 'Fixed_13B‚Üí34B',
                    'model_used': model_used,
                    'prompt_category': category,
                    'latency_ms': latency + np.random.normal(0, 30),
                    'computational_cost': cost,
                    'quality_score': quality,
                    'prompt': prompt
                })
        
        baselines['Fixed_13B‚Üí34B'] = pipeline_13_34
        
        # 13B‚Üí70B Pipeline
        pipeline_13_70 = []
        for category, prompts in prompts_by_category.items():
            for prompt in prompts:
                if category == 'simple':
                    # Use 13B for simple
                    model_used = '13B'
                    latency = self.real_latencies['13B']
                    cost = self.real_costs['13B']
                    quality = self.quality_estimates[category]['13B']
                else:
                    # Escalate to 70B for medium/complex
                    model_used = '70B'
                    latency = self.real_latencies['13B'] * 0.2 + self.real_latencies['70B']
                    cost = self.real_costs['13B'] * 0.2 + self.real_costs['70B']
                    quality = self.quality_estimates[category]['70B']
                
                pipeline_13_70.append({
                    'method': 'Fixed_13B‚Üí70B',
                    'model_used': model_used,
                    'prompt_category': category,
                    'latency_ms': latency + np.random.normal(0, 40),
                    'computational_cost': cost,
                    'quality_score': quality,
                    'prompt': prompt
                })
        
        baselines['Fixed_13B‚Üí70B'] = pipeline_13_70
        
        return baselines
    
    def process_adaptive_results(self, adaptive_results):
        """Process adaptive results for comparison."""
        processed = []
        
        for result in adaptive_results:
            processed.append({
                'method': 'Adaptive_Ours',
                'model_used': result['stage_name'],
                'prompt_category': result['prompt_category'],
                'latency_ms': result['latency_ms'],
                'computational_cost': result['computational_cost'],
                'quality_score': max(result['stage_probabilities']),  # Use max probability as quality proxy
                'prompt': result['prompt'],
                'lambda': result.get('lambda', 'unknown')
            })
        
        return processed
    
    def create_comparison_table(self, adaptive_data, baseline_data):
        """Create detailed comparison table."""
        
        print("\n" + "="*80)
        print("COMPREHENSIVE BASELINE COMPARISON")
        print("="*80)
        
        # Combine all data
        all_methods = {'Adaptive_Ours': adaptive_data}
        all_methods.update(baseline_data)
        
        # Calculate metrics for each method
        comparison_results = {}
        
        for method_name, results in all_methods.items():
            if not results:
                continue
                
            latencies = [r['latency_ms'] for r in results]
            costs = [r['computational_cost'] for r in results]
            qualities = [r['quality_score'] for r in results]
            
            comparison_results[method_name] = {
                'avg_latency': np.mean(latencies),
                'std_latency': np.std(latencies),
                'avg_cost': np.mean(costs),
                'std_cost': np.std(costs),
                'avg_quality': np.mean(qualities),
                'std_quality': np.std(qualities),
                'experiments': len(results)
            }
        
        # Print table header
        print(f"\n{'Method':<18} {'Latency(ms)':<12} {'Cost':<8} {'Quality':<10} {'Experiments':<12}")
        print("-" * 80)
        
        # Sort methods for better presentation
        method_order = ['13B_Only', '34B_Only', '70B_Only', 'Fixed_13B‚Üí34B', 'Fixed_13B‚Üí70B', 'Adaptive_Ours']
        
        for method in method_order:
            if method in comparison_results:
                data = comparison_results[method]
                print(f"{method:<18} {data['avg_latency']:<12.0f} "
                      f"{data['avg_cost']:<8.2f} {data['avg_quality']:<10.3f} "
                      f"{data['experiments']:<12}")
        
        # Calculate improvements vs baselines
        if 'Adaptive_Ours' in comparison_results:
            ours = comparison_results['Adaptive_Ours']
            
            print(f"\n{'ADAPTIVE SYSTEM IMPROVEMENTS:'}")
            print("-" * 50)
            
            for method, data in comparison_results.items():
                if method != 'Adaptive_Ours':
                    # Calculate percentage improvements
                    latency_improvement = ((data['avg_latency'] - ours['avg_latency']) / data['avg_latency']) * 100
                    cost_improvement = ((data['avg_cost'] - ours['avg_cost']) / data['avg_cost']) * 100
                    quality_change = ((ours['avg_quality'] - data['avg_quality']) / data['avg_quality']) * 100
                    
                    print(f"\nvs {method}:")
                    print(f"  Latency: {latency_improvement:+6.1f}% ({'faster' if latency_improvement > 0 else 'slower'})")
                    print(f"  Cost:    {cost_improvement:+6.1f}% ({'cheaper' if cost_improvement > 0 else 'more expensive'})")
                    print(f"  Quality: {quality_change:+6.1f}% ({'better' if quality_change > 0 else 'worse'})")
        
        # Summary statistics
        print(f"\n{'SUMMARY STATISTICS:'}")
        print("-" * 30)
        all_latencies = [comparison_results[m]['avg_latency'] for m in comparison_results]
        all_costs = [comparison_results[m]['avg_cost'] for m in comparison_results]
        all_qualities = [comparison_results[m]['avg_quality'] for m in comparison_results]
        
        print(f"Latency range: {min(all_latencies):.0f} - {max(all_latencies):.0f} ms")
        print(f"Cost range: {min(all_costs):.2f} - {max(all_costs):.2f}")
        print(f"Quality range: {min(all_qualities):.3f} - {max(all_qualities):.3f}")
        
        if 'Adaptive_Ours' in comparison_results:
            ours_rank_latency = sorted(all_latencies).index(ours['avg_latency']) + 1
            ours_rank_cost = sorted(all_costs).index(ours['avg_cost']) + 1
            ours_rank_quality = sorted(all_qualities, reverse=True).index(ours['avg_quality']) + 1
            
            print(f"\nAdaptive system ranking:")
            print(f"  Latency: #{ours_rank_latency}/{len(all_latencies)} (lower is better)")
            print(f"  Cost:    #{ours_rank_cost}/{len(all_costs)} (lower is better)")
            print(f"  Quality: #{ours_rank_quality}/{len(all_qualities)} (higher is better)")
        
        return comparison_results
    
    def create_comparison_visualizations(self, comparison_results):
        """Create visualization comparing all methods."""
        
        # Prepare data for plotting
        methods = list(comparison_results.keys())
        latencies = [comparison_results[m]['avg_latency'] for m in methods]
        costs = [comparison_results[m]['avg_cost'] for m in methods]
        qualities = [comparison_results[m]['avg_quality'] for m in methods]
        
        # Create comprehensive comparison figure
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Latency comparison
        bars1 = axes[0,0].bar(methods, latencies, color=['red' if 'Adaptive' in m else 'skyblue' for m in methods])
        axes[0,0].set_title('Average Latency Comparison', fontweight='bold', fontsize=14)
        axes[0,0].set_ylabel('Latency (ms)', fontweight='bold')
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # Highlight our method
        for i, (method, bar) in enumerate(zip(methods, bars1)):
            if 'Adaptive' in method:
                bar.set_color('#FF6B6B')
                bar.set_edgecolor('black')
                bar.set_linewidth(2)
        
        # 2. Cost comparison
        bars2 = axes[0,1].bar(methods, costs, color=['red' if 'Adaptive' in m else 'lightgreen' for m in methods])
        axes[0,1].set_title('Average Computational Cost', fontweight='bold', fontsize=14)
        axes[0,1].set_ylabel('Cost (normalized)', fontweight='bold')
        axes[0,1].tick_params(axis='x', rotation=45)
        
        for i, (method, bar) in enumerate(zip(methods, bars2)):
            if 'Adaptive' in method:
                bar.set_color('#FF6B6B')
                bar.set_edgecolor('black')
                bar.set_linewidth(2)
        
        # 3. Quality comparison
        bars3 = axes[1,0].bar(methods, qualities, color=['red' if 'Adaptive' in m else 'orange' for m in methods])
        axes[1,0].set_title('Average Quality Score', fontweight='bold', fontsize=14)
        axes[1,0].set_ylabel('Quality Score', fontweight='bold')
        axes[1,0].tick_params(axis='x', rotation=45)
        
        for i, (method, bar) in enumerate(zip(methods, bars3)):
            if 'Adaptive' in method:
                bar.set_color('#FF6B6B')
                bar.set_edgecolor('black')
                bar.set_linewidth(2)
        
        # 4. Scatter plot: Cost vs Quality (Pareto frontier)
        for i, method in enumerate(methods):
            color = '#FF6B6B' if 'Adaptive' in method else 'blue'
            marker = 'o' if 'Adaptive' in method else 's'
            size = 150 if 'Adaptive' in method else 100
            
            axes[1,1].scatter(costs[i], qualities[i], c=color, s=size, marker=marker, 
                            label=method, alpha=0.8, edgecolors='black')
        
        axes[1,1].set_xlabel('Computational Cost', fontweight='bold')
        axes[1,1].set_ylabel('Quality Score', fontweight='bold')
        axes[1,1].set_title('Cost vs Quality Trade-off', fontweight='bold', fontsize=14)
        axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save figure
        fig_path = "figures/baseline_comparison.png"
        Path("figures").mkdir(exist_ok=True)
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        print(f"\n‚úÖ Baseline comparison visualization saved to: {fig_path}")
        plt.show()

def main():
    """Run comprehensive baseline analysis."""
    analyzer = BaselineAnalyzer()
    
    # Load adaptive results
    adaptive_results = analyzer.load_adaptive_results()
    if not adaptive_results:
        print("‚ùå Could not load adaptive results")
        return
    
    # Filter to Œª=5.0 results for fair comparison
    lambda_5_results = [r for r in adaptive_results if r.get('lambda') == 5.0]
    
    print(f"üìä Analyzing {len(lambda_5_results)} adaptive results (Œª=5.0)")
    
    # Process adaptive data
    adaptive_data = analyzer.process_adaptive_results(lambda_5_results)
    
    # Simulate baseline data
    baseline_data = analyzer.simulate_baseline_results(lambda_5_results)
    
    print(f"üî¨ Generated {sum(len(results) for results in baseline_data.values())} baseline comparisons")
    
    # Create comparison
    comparison_results = analyzer.create_comparison_table(adaptive_data, baseline_data)
    
    # Create visualizations
    analyzer.create_comparison_visualizations(comparison_results)
    
    print(f"\nüéâ Baseline comparison analysis complete!")

if __name__ == "__main__":
    main()