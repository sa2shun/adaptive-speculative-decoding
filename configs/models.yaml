# Model configuration for Adaptive Speculative Decoding

models:
  8b:
    name: "meta-llama/Llama-3.2-8B"
    size: "8b"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.8
    quantization:
      enabled: true
      method: "nf4"
      compute_dtype: "float16"
    cost_per_token: 1.0
    
  13b:
    name: "meta-llama/Llama-3-13B"  # Placeholder
    size: "13b"
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.8
    quantization:
      enabled: true
      method: "nf4"
      compute_dtype: "float16"
    cost_per_token: 1.6
    
  34b:
    name: "codellama/CodeLlama-34b-hf"  # Using as substitute
    size: "34b"
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.85
    quantization:
      enabled: true
      method: "nf4"
      compute_dtype: "float16"
    cost_per_token: 4.2
    
  70b:
    name: "meta-llama/Llama-3.1-70B"
    size: "70b"
    tensor_parallel_size: 4
    gpu_memory_utilization: 0.9
    quantization:
      enabled: true
      method: "nf4"
      compute_dtype: "float16"
      use_double_quant: true
    cost_per_token: 8.8

# vLLM configuration
vllm:
  max_num_batched_tokens: 4096
  max_num_seqs: 256
  enable_chunked_prefill: true
  enable_prefix_caching: true
  
# Generation parameters
generation:
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1