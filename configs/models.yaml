generation:
  max_tokens: 512
  repetition_penalty: 1.1
  temperature: 0.7
  top_k: 50
  top_p: 0.9
models:
  13b:
    cost_per_token: 1.6
    gpu_memory_utilization: 0.9
    name: meta-llama/Llama-3.1-8B-Instruct
    quantization:
      compute_dtype: float16
      enabled: false
      method: null
    size: 13b
    tensor_parallel_size: 2
  34b:
    cost_per_token: 4.2
    gpu_memory_utilization: 0.9
    name: codellama/CodeLlama-34b-hf
    quantization:
      compute_dtype: float16
      enabled: false
      method: null
    size: 34b
    tensor_parallel_size: 2
  70b:
    cost_per_token: 8.8
    gpu_memory_utilization: 0.95
    name: meta-llama/Llama-3.1-70B-Instruct
    quantization:
      compute_dtype: float16
      enabled: false
      method: null
    size: 70b
    tensor_parallel_size: 4
  8b:
    cost_per_token: 1.0
    gpu_memory_utilization: 0.9
    name: meta-llama/Llama-3.1-8B
    quantization:
      compute_dtype: float16
      enabled: false
      method: null
    size: 8b
    tensor_parallel_size: 1
vllm:
  enable_chunked_prefill: true
  enable_prefix_caching: true
  max_num_batched_tokens: 4096
  max_num_seqs: 256
