# Qwen3 Model Hierarchy Configuration
# Full-precision models for research-grade experiments
# NO quantization to maintain research quality standards

models:
  stages:
    - name: "qwen3-7b"
      model_path: "Qwen/Qwen3-7B-Instruct"
      size_label: "7b"
      tensor_parallel_size: 1
      gpu_memory_fraction: 0.9
      quantization: null  # Full precision - NO quantization
      max_model_len: 4096
      dtype: "bfloat16"
      gpu_ids: [0]
      # Cost measured from actual inference (to be calibrated)
      base_latency_ms: null  # Will be measured during calibration
      
    - name: "qwen3-14b"
      model_path: "Qwen/Qwen3-14B-Instruct"
      size_label: "14b"
      tensor_parallel_size: 1  # Can fit on single A100
      gpu_memory_fraction: 0.9
      quantization: null  # Full precision - NO quantization
      max_model_len: 4096
      dtype: "bfloat16"
      gpu_ids: [1]
      # Cost measured from actual inference (to be calibrated)
      base_latency_ms: null  # Will be measured during calibration
      
    - name: "qwen3-32b"
      model_path: "Qwen/Qwen3-32B-Instruct"
      size_label: "32b"
      tensor_parallel_size: 2
      gpu_memory_fraction: 0.9
      quantization: null  # Full precision - NO quantization
      max_model_len: 4096
      dtype: "bfloat16"
      gpu_ids: [2, 3]
      # Cost measured from actual inference (to be calibrated)
      base_latency_ms: null  # Will be measured during calibration
      
    - name: "qwen3-72b"
      model_path: "Qwen/Qwen3-72B-Instruct"
      size_label: "72b"
      tensor_parallel_size: 4
      gpu_memory_fraction: 0.9
      quantization: null  # Full precision - NO quantization
      max_model_len: 4096
      dtype: "bfloat16"
      gpu_ids: [4, 5, 6, 7]
      # Cost measured from actual inference (to be calibrated)
      base_latency_ms: null  # Will be measured during calibration

# vLLM settings for efficient inference
vllm:
  enable_chunked_prefill: true
  enable_prefix_caching: true
  max_num_batched_tokens: 4096
  max_num_seqs: 256
  trust_remote_code: false
  revision: "main"
  tokenizer_mode: "auto"
  load_format: "auto"
  enforce_eager: false
  disable_custom_all_reduce: false

# Quality predictor configuration
predictor:
  architecture: "mlp"
  input_dim: 128  # Expanded feature set
  hidden_layers: [256, 128, 64]
  output_dim: 1
  activation: "relu"
  dropout: 0.2
  batch_normalization: true
  
# Evaluation datasets - research scale
evaluation:
  datasets:
    - name: "mmlu"
      subset: "test"
      max_samples: 2000  # Research scale requirement
      
    - name: "humaneval"  
      subset: "test"
      max_samples: 1000  # Expanded from 164
      
    - name: "gsm8k"
      subset: "test"
      max_samples: 1319  # Full dataset
      
    - name: "truthfulqa"
      subset: "validation"
      max_samples: 1000
      
  metrics:
    - "accuracy"
    - "pass_at_1" 
    - "bleu"
    - "rouge"
    - "bertscore"
    - "inference_speedup"
    - "wall_clock_time"
    - "gpu_utilization"
    
# Experimental parameters - comprehensive sweep
experiment:
  lambda_values: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]  # 6 values as specified
  num_seeds: 5  # Multiple runs for statistical significance
  confidence_level: 0.95
  significance_level: 0.01
  
# Cost profiling configuration
cost_profiling:
  enable: true
  calibration_samples: 100  # For measuring real latencies
  batch_sizes: [1, 4, 8, 16]
  sequence_lengths: [128, 256, 512, 1024]
  warmup_iterations: 10
  measurement_iterations: 50
  
# Storage paths
storage:
  models: "/raid/$USER/adaptive-sd-models/"
  training_data: "/raid/$USER/adaptive-sd-training-data/"
  evaluation_data: "/raid/$USER/adaptive-sd-eval-data/"
  results: "/raid/$USER/adaptive-sd-results/"
  logs: "/raid/$USER/adaptive-sd-logs/"