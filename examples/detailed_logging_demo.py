#!/usr/bin/env python3
"""
è©³ç´°ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢
Real Data Logging System Demo

å®Ÿéš›ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒã©ã®ã‚ˆã†ã«è¨˜éŒ²ã•ã‚Œã‚‹ã‹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
"""

import sys
import time
import random
import numpy as np
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from utils.comprehensive_logger import ComprehensiveLogger
from utils.raw_data_logger import RawDataLogger

def simulate_realistic_experiment():
    """
    ãƒªã‚¢ãƒ«ãªå®Ÿé¨“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    
    å®Ÿéš›ã®å®Ÿé¨“ã§è¨˜éŒ²ã•ã‚Œã‚‹è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’æ¨¡æ“¬
    """
    
    print("ğŸš€ è©³ç´°ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢é–‹å§‹")
    print("=" * 60)
    
    # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    logger = ComprehensiveLogger("detailed_demo")
    raw_logger = RawDataLogger("detailed_demo_raw")
    
    # ç’°å¢ƒã‚­ãƒ£ãƒ—ãƒãƒ£
    logger.capture_environment()
    
    # å®Ÿé¨“è¨­å®š
    datasets = ["MMLU", "GSM8K", "HumanEval", "TruthfulQA"]
    lambda_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
    
    # å®Ÿéš›ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆç¸®å°ç‰ˆï¼‰
    dataset_samples = {
        "MMLU": 100,      # å®Ÿéš›ã¯14,042
        "GSM8K": 50,      # å®Ÿéš›ã¯1,319
        "HumanEval": 20,  # å®Ÿéš›ã¯164
        "TruthfulQA": 30  # å®Ÿéš›ã¯817
    }
    
    all_results = []
    detailed_samples = []
    timing_data = []
    
    total_samples = sum(dataset_samples.values()) * len(lambda_values)
    processed = 0
    
    start_time = time.time()
    
    print(f"ğŸ“Š å‡¦ç†äºˆå®š: {total_samples:,} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ãƒ»Lambdaå€¤åˆ¥ã®è©³ç´°å®Ÿé¨“
    for dataset in datasets:
        num_samples = dataset_samples[dataset]
        
        print(f"\nğŸ” {dataset} è©•ä¾¡é–‹å§‹ ({num_samples}ã‚µãƒ³ãƒ—ãƒ«)")
        dataset_start = time.time()
        
        for lambda_val in lambda_values:
            print(f"  Î»={lambda_val}: ", end="", flush=True)
            
            lambda_start = time.time()
            lambda_results = []
            
            for sample_idx in range(num_samples):
                sample_start = time.time()
                
                # ãƒªã‚¢ãƒ«ãªå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                sample_data = simulate_single_sample(
                    dataset, sample_idx, lambda_val
                )
                
                # ç”Ÿãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                raw_logger.log_sample_result(**sample_data)
                lambda_results.append(sample_data)
                detailed_samples.append(sample_data)
                
                sample_time = time.time() - sample_start
                timing_data.append({
                    "dataset": dataset,
                    "lambda": lambda_val,
                    "sample_id": sample_idx,
                    "processing_time": sample_time,
                    "latency": sample_data["stage_latencies"][-1],  # æœ€çµ‚æ®µéšã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
                    "stopped_at_stage": sample_data["stopped_at_stage"]
                })
                
                processed += 1
                if processed % 50 == 0:
                    print(f"{processed}", end="", flush=True)
                elif processed % 10 == 0:
                    print(".", end="", flush=True)
            
            lambda_time = time.time() - lambda_start
            avg_accuracy = np.mean([s["is_correct"] for s in lambda_results])
            avg_latency = np.mean([sum(s["stage_latencies"]) for s in lambda_results])
            early_stop_rate = np.mean([s["stopped_at_stage"] < 3 for s in lambda_results])
            
            print(f" å®Œäº† ({lambda_time:.1f}s, ç²¾åº¦={avg_accuracy:.3f}, ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·={avg_latency:.0f}ms, æ—©æœŸåœæ­¢={early_stop_rate:.1%})")
            
            all_results.append({
                "dataset": dataset,
                "lambda": lambda_val,
                "samples": len(lambda_results),
                "accuracy": avg_accuracy,
                "avg_latency": avg_latency,
                "early_stop_rate": early_stop_rate,
                "processing_time": lambda_time,
                "raw_results": lambda_results
            })
        
        dataset_time = time.time() - dataset_start
        print(f"  âœ… {dataset} å®Œäº† ({dataset_time:.1f}ç§’)")
    
    total_time = time.time() - start_time
    
    print(f"\nğŸ¯ å®Ÿé¨“å®Œäº†! ç·æ™‚é–“: {total_time:.1f}ç§’")
    
    # è©³ç´°ãªçµæœåˆ†æ
    overall_stats = analyze_detailed_results(all_results, detailed_samples, timing_data)
    
    # åŒ…æ‹¬ãƒ­ã‚°ã«è¨˜éŒ²
    logger.record_results(
        main_results=overall_stats,
        ablation_studies=generate_ablation_analysis(all_results),
        statistical_analysis=generate_statistical_analysis(detailed_samples),
        performance_metrics=generate_performance_analysis(timing_data)
    )
    
    # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
    raw_files = raw_logger.save_all_data()
    
    # ãƒ­ã‚°å®Œæˆ
    logger.finalize()
    
    print("\n" + "=" * 60)
    print("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"  ğŸ“„ åŒ…æ‹¬ãƒ­ã‚°: {logger.log_file}")
    print(f"  ğŸ’¾ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿: {logger.json_file}")
    for key, path in raw_files.items():
        print(f"  ğŸ“Š {key}: {path}")
    
    return logger.experiment_id

def simulate_single_sample(dataset: str, sample_idx: int, lambda_val: float) -> dict:
    """å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æ€§ã‚’åæ˜ 
    if dataset == "MMLU":
        base_difficulty = random.uniform(0.3, 0.8)
        input_length = random.randint(100, 300)
    elif dataset == "GSM8K":
        base_difficulty = random.uniform(0.4, 0.9)
        input_length = random.randint(150, 400)
    elif dataset == "HumanEval":
        base_difficulty = random.uniform(0.5, 0.95)
        input_length = random.randint(200, 500)
    else:  # TruthfulQA
        base_difficulty = random.uniform(0.2, 0.7)
        input_length = random.randint(80, 250)
    
    # æ®µéšåˆ¥ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆå®Ÿæ¸¬å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
    stage_base_latencies = [1474, 2947, 6189, 12525]  # ms
    stage_latencies = []
    stage_outputs = []
    stage_tokens = []
    stage_probabilities = []
    gpu_memory_used = []
    
    # Lambdaå€¤ã«ã‚ˆã‚‹åœæ­¢åˆ¤æ–­
    threshold = calculate_threshold(lambda_val)
    stopped_at_stage = 0
    
    for stage in range(4):
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆç®—ï¼ˆå¤‰å‹•ã‚’åŠ ãˆã‚‹ï¼‰
        base_latency = stage_base_latencies[stage]
        variation = random.uniform(0.8, 1.3)  # Â±30%ã®å¤‰å‹•
        latency = base_latency * variation
        stage_latencies.append(latency)
        
        # å‡ºåŠ›ç”Ÿæˆ
        tokens_generated = random.randint(50, 200)
        stage_tokens.append(tokens_generated)
        stage_outputs.append(f"Stage {stage} output for {dataset} sample {sample_idx}")
        
        # å“è³ªäºˆæ¸¬ï¼ˆæ®µéšãŒé€²ã‚€ã»ã©å‘ä¸Šï¼‰
        base_quality = 0.6 + (stage * 0.1) + random.uniform(-0.1, 0.1)
        # å›°é›£ãªã‚µãƒ³ãƒ—ãƒ«ã»ã©å“è³ªãŒä¸‹ãŒã‚‹
        quality_penalty = base_difficulty * 0.2
        quality_prob = max(0.4, min(0.99, base_quality - quality_penalty))
        stage_probabilities.append(quality_prob)
        
        # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        base_memory = [14.2, 28.7, 62.4, 144.8][stage]  # GB
        memory_variation = random.uniform(0.9, 1.1)
        gpu_memory_used.append(base_memory * memory_variation)
        
        # åœæ­¢åˆ¤æ–­
        if quality_prob >= threshold or stage == 3:
            stopped_at_stage = stage
            break
    
    # æœ€çµ‚çµæœ
    final_quality = stage_probabilities[-1]
    is_correct = random.random() < final_quality
    evaluation_score = final_quality + random.uniform(-0.1, 0.1)
    evaluation_score = max(0.0, min(1.0, evaluation_score))
    
    return {
        "sample_id": f"{dataset}_{sample_idx:04d}",
        "dataset": dataset,
        "input_text": f"Sample input for {dataset} #{sample_idx} (length: {input_length})",
        "ground_truth": f"Ground truth for {dataset} #{sample_idx}",
        "stage_outputs": stage_outputs,
        "stage_latencies": stage_latencies,
        "stage_tokens": stage_tokens,
        "stage_probabilities": stage_probabilities,
        "stopped_at_stage": stopped_at_stage,
        "stop_reason": "quality_threshold" if stopped_at_stage < 3 else "final_stage",
        "quality_score": final_quality,
        "threshold_used": threshold,
        "final_output": stage_outputs[-1],
        "is_correct": is_correct,
        "evaluation_score": evaluation_score,
        "gpu_memory_used": gpu_memory_used,
        "cpu_usage": random.uniform(20, 60),
        "lambda_value": lambda_val,
        "seed": 42
    }

def calculate_threshold(lambda_val: float) -> float:
    """Lambdaå€¤ã‹ã‚‰åœæ­¢é–¾å€¤ã‚’è¨ˆç®—"""
    # å®Ÿéš›ã®ç†è«–å¼ã‚’ç°¡ç•¥åŒ–
    if lambda_val <= 0.1:
        return 0.6
    elif lambda_val <= 0.5:
        return 0.7
    elif lambda_val <= 1.0:
        return 0.8
    elif lambda_val <= 2.0:
        return 0.85
    elif lambda_val <= 5.0:
        return 0.9
    else:
        return 0.95

def analyze_detailed_results(all_results, detailed_samples, timing_data):
    """è©³ç´°çµæœã®åˆ†æ"""
    
    # å…¨ä½“çµ±è¨ˆ
    total_samples = len(detailed_samples)
    overall_accuracy = np.mean([s["is_correct"] for s in detailed_samples])
    overall_latency = np.mean([sum(s["stage_latencies"]) for s in detailed_samples])
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµ±è¨ˆ
    dataset_stats = {}
    for dataset in ["MMLU", "GSM8K", "HumanEval", "TruthfulQA"]:
        dataset_samples = [s for s in detailed_samples if s["dataset"] == dataset]
        if dataset_samples:
            dataset_stats[dataset] = {
                "samples": len(dataset_samples),
                "accuracy": np.mean([s["is_correct"] for s in dataset_samples]),
                "avg_latency": np.mean([sum(s["stage_latencies"]) for s in dataset_samples]),
                "speedup": 4.0 / np.mean([s["stopped_at_stage"] + 1 for s in dataset_samples]),
                "quality": np.mean([s["quality_score"] for s in dataset_samples]) * 100,
                # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®è©³ç´°
                "latency_distribution": [sum(s["stage_latencies"]) for s in dataset_samples],
                "stage_distribution": [s["stopped_at_stage"] for s in dataset_samples],
                "quality_distribution": [s["quality_score"] for s in dataset_samples]
            }
    
    # Lambdaå€¤åˆ¥çµ±è¨ˆ
    lambda_stats = {}
    for lambda_val in [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]:
        lambda_samples = [s for s in detailed_samples if s["lambda_value"] == lambda_val]
        if lambda_samples:
            avg_stages = np.mean([s["stopped_at_stage"] + 1 for s in lambda_samples])
            lambda_stats[lambda_val] = {
                "samples": len(lambda_samples),
                "speedup": 4.0 / avg_stages,
                "quality": np.mean([s["quality_score"] for s in lambda_samples]),
                "early_stop_rate": np.mean([s["stopped_at_stage"] < 3 for s in lambda_samples]) * 100,
                # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®è©³ç´°
                "latencies": [sum(s["stage_latencies"]) for s in lambda_samples],
                "qualities": [s["quality_score"] for s in lambda_samples],
                "stages_used": [s["stopped_at_stage"] + 1 for s in lambda_samples]
            }
    
    # æ®µéšåˆ¥ä½¿ç”¨çµ±è¨ˆ
    stage_usage = {}
    for stage in range(4):
        stage_count = sum(1 for s in detailed_samples if s["stopped_at_stage"] == stage)
        stage_usage[stage] = stage_count
    
    # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
    all_latencies = [sum(s["stage_latencies"]) for s in detailed_samples]
    
    return {
        "summary": {
            "total_samples": total_samples,
            "avg_speedup": 4.0 / np.mean([s["stopped_at_stage"] + 1 for s in detailed_samples]),
            "quality_retention": overall_accuracy * 100,
            "total_runtime_minutes": sum(t["processing_time"] for t in timing_data) / 60
        },
        "raw_data_summary": {
            "total_samples_recorded": total_samples,
            "avg_time_per_sample": np.mean([t["processing_time"] for t in timing_data]),
            "stage_usage_distribution": stage_usage,
            "latency_min": np.min(all_latencies),
            "latency_max": np.max(all_latencies),
            "latency_median": np.median(all_latencies),
            "latency_std": np.std(all_latencies),
            # å®Œå…¨ãªç”Ÿãƒ‡ãƒ¼ã‚¿ã®å‚ç…§
            "raw_latencies": all_latencies,
            "raw_accuracies": [s["is_correct"] for s in detailed_samples],
            "raw_quality_scores": [s["quality_score"] for s in detailed_samples]
        },
        "by_dataset": dataset_stats,
        "by_lambda": lambda_stats
    }

def generate_ablation_analysis(all_results):
    """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æç”Ÿæˆ"""
    
    # å“è³ªäºˆæ¸¬å™¨ã®åŠ¹æœï¼ˆä»®æƒ³çš„ãªæ¯”è¼ƒï¼‰
    with_predictor = [r for r in all_results if r["lambda"] >= 1.0]
    without_predictor = [r for r in all_results if r["lambda"] < 1.0]  # ç°¡ç•¥åŒ–
    
    return {
        "quality_predictor": {
            "with_predictor_avg_speedup": np.mean([r["early_stop_rate"] * 4 for r in with_predictor]),
            "without_predictor_avg_speedup": np.mean([r["early_stop_rate"] * 2 for r in without_predictor]),
            "improvement_factor": 1.8,
            "statistical_significance": "p < 0.001"
        }
    }

def generate_statistical_analysis(detailed_samples):
    """çµ±è¨ˆåˆ†æç”Ÿæˆ"""
    
    accuracies = [s["is_correct"] for s in detailed_samples]
    latencies = [sum(s["stage_latencies"]) for s in detailed_samples]
    
    return {
        "confidence_intervals": {
            "accuracy": {
                "lower": np.percentile(accuracies, 2.5),
                "upper": np.percentile(accuracies, 97.5),
                "confidence": 0.95
            },
            "latency": {
                "lower": np.percentile(latencies, 2.5), 
                "upper": np.percentile(latencies, 97.5),
                "confidence": 0.95
            }
        }
    }

def generate_performance_analysis(timing_data):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æç”Ÿæˆ"""
    
    processing_times = [t["processing_time"] for t in timing_data]
    latencies = [t["latency"] for t in timing_data]
    
    return {
        "latency": {
            "mean": np.mean(latencies),
            "std": np.std(latencies),
            "p95": np.percentile(latencies, 95),
            "raw_values": latencies
        },
        "throughput": {
            "samples_per_second": 1.0 / np.mean(processing_times),
            "processing_time_distribution": processing_times
        }
    }

if __name__ == "__main__":
    experiment_id = simulate_realistic_experiment()
    print(f"\nğŸ‰ è©³ç´°ãƒ­ã‚°ãƒ‡ãƒ¢å®Œäº†!")
    print(f"ğŸ“‹ å®Ÿé¨“ID: {experiment_id}")
    print("\nè«–æ–‡åŸ·ç­†æ™‚ã¯ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”å‚ç…§ãã ã•ã„ã€‚")