#!/usr/bin/env python3
"""
Comprehensive Experiment Logger for Adaptive Speculative Decoding Research
å®Œå…¨ãªå®Ÿé¨“ç’°å¢ƒãƒ»çµæœè¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ 

è«–æ–‡åŸ·ç­†æ™‚ã®å‚è€ƒè³‡æ–™ã¨ã—ã¦ã€ã™ã¹ã¦ã®å®Ÿé¨“æƒ…å ±ã‚’ä¸€å…ƒç®¡ç†
"""

import json
import yaml
import logging
import datetime
import platform
import psutil
import torch
import subprocess
import sys
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import numpy as np
import pandas as pd
# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯å¾Œã§è¨­å®š
# from .raw_data_logger import RawDataLogger, SampleResult

@dataclass
class ExperimentEnvironment:
    """å®Ÿé¨“ç’°å¢ƒã®å®Œå…¨è¨˜éŒ²"""
    timestamp: str
    experiment_id: str
    git_commit: str
    git_branch: str
    
    # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç’°å¢ƒ
    hardware: Dict[str, Any]
    
    # ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç’°å¢ƒ  
    software: Dict[str, Any]
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    models: Dict[str, Any]
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    datasets: Dict[str, Any]
    
    # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parameters: Dict[str, Any]

@dataclass  
class ExperimentResults:
    """å®Ÿé¨“çµæœã®å®Œå…¨è¨˜éŒ²"""
    timestamp: str
    experiment_id: str
    
    # ãƒ¡ã‚¤ãƒ³çµæœ
    main_results: Dict[str, Any]
    
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶
    ablation_studies: Dict[str, Any]
    
    # çµ±è¨ˆåˆ†æ
    statistical_analysis: Dict[str, Any]
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
    performance_metrics: Dict[str, Any]
    
    # ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Š
    errors_warnings: List[str]

class ComprehensiveLogger:
    """
    è«–æ–‡åŸ·ç­†ç”¨ã®åŒ…æ‹¬çš„å®Ÿé¨“ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ 
    
    beginner_guide_japanese.texã®å½¢å¼ã‚’å‚è€ƒã«ã€
    å®Ÿé¨“ç’°å¢ƒã®å®Œå…¨å†ç¾ã¨çµæœã®è©³ç´°è¨˜éŒ²ã‚’å®Ÿç¾
    """
    
    def __init__(self, experiment_name: str, output_dir: str = "./logs/"):
        self.experiment_name = experiment_name
        self.experiment_id = f"{experiment_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.output_dir = Path(output_dir).expanduser()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.log_file = self.output_dir / f"{self.experiment_id}_comprehensive.md"
        self.json_file = self.output_dir / f"{self.experiment_id}_data.json"
        
        # ç’°å¢ƒãƒ»çµæœè¨˜éŒ²
        self.environment: Optional[ExperimentEnvironment] = None
        self.results: Optional[ExperimentResults] = None
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ­ã‚¬ãƒ¼ï¼ˆçµ±åˆæ¸ˆã¿ï¼‰
        try:
            from .raw_data_logger import RawDataLogger
            self.raw_data_logger = RawDataLogger(
                self.experiment_id, 
                str(self.output_dir / "raw_data"),
                enable_progress_tracking=False  # åŒ…æ‹¬ãƒ­ã‚°ã¨ã®é‡è¤‡ã‚’é¿ã‘ã‚‹
            )
        except ImportError:
            self.raw_data_logger = None
        
        # ãƒ­ã‚°é–‹å§‹
        self._initialize_logging()
        
    def _initialize_logging(self):
        """ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        # Markdownãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = f"""# é©å¿œçš„æ¨æ¸¬ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿé¨“è¨˜éŒ²
## å®Ÿé¨“ID: {self.experiment_id}
## å®Ÿé¨“å: {self.experiment_name}
## ä½œæˆæ—¥æ™‚: {datetime.datetime.now().isoformat()}

---

**ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦:**
- æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã¯å®Ÿé¨“ç’°å¢ƒã®å®Œå…¨å†ç¾ã¨çµæœã®è©³ç´°è¨˜éŒ²ã‚’ç›®çš„ã¨ã™ã‚‹
- è«–æ–‡åŸ·ç­†æ™‚ã®å‚è€ƒè³‡æ–™ã¨ã—ã¦è¨­è¨ˆ
- ã™ã¹ã¦ã®å®Ÿé¨“æƒ…å ±ãŒä¸€å…ƒç®¡ç†ã•ã‚Œã¦ã„ã‚‹

---

"""
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write(header)
    
    def capture_environment(self, config_files: List[str] = None) -> ExperimentEnvironment:
        """
        å®Ÿé¨“ç’°å¢ƒã®å®Œå…¨ã‚­ãƒ£ãƒ—ãƒãƒ£
        
        Args:
            config_files: è¨˜éŒ²ã™ã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        print("ğŸ” å®Ÿé¨“ç’°å¢ƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­...")
        
        # Gitæƒ…å ±å–å¾—
        try:
            git_commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()
            git_branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode().strip()
        except:
            git_commit = "unknown"
            git_branch = "unknown"
        
        # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±
        hardware = {
            "cpu": {
                "model": platform.processor(),
                "cores": psutil.cpu_count(logical=False),
                "threads": psutil.cpu_count(logical=True),
                "frequency": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None
            },
            "memory": {
                "total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
                "available_gb": round(psutil.virtual_memory().available / (1024**3), 2)
            },
            "gpu": self._get_gpu_info(),
            "storage": self._get_storage_info()
        }
        
        # ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢æƒ…å ±
        software = {
            "os": {
                "system": platform.system(),
                "version": platform.version(),
                "release": platform.release()
            },
            "python": {
                "version": sys.version,
                "executable": sys.executable
            },
            "packages": self._get_package_versions()
        }
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        models_config = self._load_config_file("configs/qwen2.5_models.yaml")
        datasets_config = self._load_config_file("configs/evaluation.yaml")
        
        # è¿½åŠ ã®è©³ç´°æƒ…å ±å–å¾—
        additional_details = self._get_additional_experiment_details()
        
        # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        parameters = {
            "lambda_values": models_config.get("experiment", {}).get("lambda_values", []),
            "num_seeds": models_config.get("experiment", {}).get("num_seeds", 5),
            "confidence_level": models_config.get("experiment", {}).get("confidence_level", 0.95)
        }
        
        self.environment = ExperimentEnvironment(
            timestamp=datetime.datetime.now().isoformat(),
            experiment_id=self.experiment_id,
            git_commit=git_commit,
            git_branch=git_branch,
            hardware=hardware,
            software=software,
            models=models_config,
            datasets=datasets_config,
            parameters=parameters
        )
        
        self._write_environment_section()
        print("âœ… å®Ÿé¨“ç’°å¢ƒã‚­ãƒ£ãƒ—ãƒãƒ£å®Œäº†")
        return self.environment
    
    def _get_gpu_info(self) -> List[Dict]:
        """GPUæƒ…å ±å–å¾—"""
        gpu_info = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_info.append({
                    "id": i,
                    "name": torch.cuda.get_device_name(i),
                    "memory_total_gb": round(torch.cuda.get_device_properties(i).total_memory / (1024**3), 2),
                    "memory_reserved_gb": round(torch.cuda.memory_reserved(i) / (1024**3), 2),
                    "compute_capability": f"{torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}"
                })
        return gpu_info
    
    def _get_storage_info(self) -> Dict:
        """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æƒ…å ±å–å¾—"""
        storage = {}
        for partition in psutil.disk_partitions():
            try:
                usage = psutil.disk_usage(partition.mountpoint)
                storage[partition.mountpoint] = {
                    "total_gb": round(usage.total / (1024**3), 2),
                    "free_gb": round(usage.free / (1024**3), 2),
                    "used_percent": round((usage.used / usage.total) * 100, 1)
                }
            except:
                pass
        return storage
    
    def _get_package_versions(self) -> Dict:
        """ä¸»è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å–å¾—"""
        packages = {}
        important_packages = ['torch', 'transformers', 'vllm', 'numpy', 'pandas', 'scikit-learn']
        
        for pkg in important_packages:
            try:
                package = __import__(pkg)
                packages[pkg] = getattr(package, '__version__', 'unknown')
            except ImportError:
                packages[pkg] = 'not_installed'
        
        return packages
    
    def _get_additional_experiment_details(self) -> Dict:
        """è¿½åŠ ã®å®Ÿé¨“è©³ç´°æƒ…å ±å–å¾—"""
        details = {}
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        model_verification = {}
        model_paths = [
            "/raid/sasaki/adaptive-sd-models/qwen2.5-7b/",
            "/raid/sasaki/adaptive-sd-models/qwen2.5-14b/", 
            "/raid/sasaki/adaptive-sd-models/qwen2.5-32b/",
            "/raid/sasaki/adaptive-sd-models/qwen2.5-72b/"
        ]
        
        for path in model_paths:
            model_name = Path(path).name
            if Path(path).exists():
                safetensors_files = list(Path(path).glob("*.safetensors"))
                config_files = list(Path(path).glob("config.json"))
                model_verification[model_name] = {
                    "path": path,
                    "exists": True,
                    "safetensors_count": len(safetensors_files),
                    "has_config": len(config_files) > 0,
                    "total_size_gb": self._get_directory_size(path)
                }
            else:
                model_verification[model_name] = {
                    "path": path,
                    "exists": False
                }
        
        details["model_verification"] = model_verification
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°
        dataset_info = {
            "mmlu": {"expected_samples": 14042, "type": "multiple_choice", "subjects": 57},
            "gsm8k": {"expected_samples": 1319, "type": "math_reasoning", "grade_level": "K-8"},
            "humaneval": {"expected_samples": 164, "type": "code_generation", "language": "python"},
            "truthfulqa": {"expected_samples": 817, "type": "truthfulness", "categories": 38}
        }
        details["dataset_info"] = dataset_info
        
        # å®Ÿé¨“ç’°å¢ƒã®æ¤œè¨¼
        environment_checks = {
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "free_disk_space_gb": round(psutil.disk_usage('.').free / (1024**3), 2),
            "available_memory_gb": round(psutil.virtual_memory().available / (1024**3), 2)
        }
        details["environment_checks"] = environment_checks
        
        return details
    
    def _get_directory_size(self, path: str) -> float:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚µã‚¤ã‚ºã‚’å–å¾—ï¼ˆGBå˜ä½ï¼‰"""
        try:
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(path):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total_size += os.path.getsize(filepath)
            return round(total_size / (1024**3), 2)
        except:
            return 0.0
    
    def _load_config_file(self, filepath: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                if filepath.endswith('.yaml') or filepath.endswith('.yml'):
                    return yaml.safe_load(f)
                elif filepath.endswith('.json'):
                    return json.load(f)
        except Exception as e:
            return {"error": f"Failed to load {filepath}: {str(e)}"}
        return {}
    
    def _write_environment_section(self):
        """ç’°å¢ƒæƒ…å ±ã‚’Markdownã«è¨˜éŒ²"""
        if not self.environment:
            return
            
        content = f"""
## 1. å®Ÿé¨“ç’°å¢ƒ

### 1.1 åŸºæœ¬æƒ…å ±
- **å®Ÿé¨“ID**: {self.environment.experiment_id}
- **å®Ÿè¡Œæ—¥æ™‚**: {self.environment.timestamp}
- **Git ã‚³ãƒŸãƒƒãƒˆ**: `{self.environment.git_commit}`
- **Git ãƒ–ãƒ©ãƒ³ãƒ**: `{self.environment.git_branch}`

### 1.2 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆ

#### CPU
- **ãƒ¢ãƒ‡ãƒ«**: {self.environment.hardware['cpu']['model']}
- **ç‰©ç†ã‚³ã‚¢æ•°**: {self.environment.hardware['cpu']['cores']}
- **è«–ç†ã‚³ã‚¢æ•°**: {self.environment.hardware['cpu']['threads']}

#### ãƒ¡ãƒ¢ãƒª
- **ç·å®¹é‡**: {self.environment.hardware['memory']['total_gb']} GB
- **ä½¿ç”¨å¯èƒ½**: {self.environment.hardware['memory']['available_gb']} GB

#### GPUæ§‹æˆ
"""
        
        for gpu in self.environment.hardware['gpu']:
            content += f"""
- **GPU {gpu['id']}**: {gpu['name']}
  - ãƒ¡ãƒ¢ãƒª: {gpu['memory_total_gb']} GB
  - Compute Capability: {gpu['compute_capability']}
"""
        
        content += f"""
### 1.3 ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç’°å¢ƒ

#### ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
- **OS**: {self.environment.software['os']['system']} {self.environment.software['os']['release']}

#### Pythonç’°å¢ƒ
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: {self.environment.software['python']['version'].split()[0]}
- **å®Ÿè¡Œãƒ‘ã‚¹**: `{self.environment.software['python']['executable']}`

#### ä¸»è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
"""
        
        for pkg, version in self.environment.software['packages'].items():
            content += f"- **{pkg}**: {version}\n"
        
        content += f"""
### 1.4 ãƒ¢ãƒ‡ãƒ«è¨­å®š

#### Qwen2.5 4æ®µéšéšå±¤
"""
        
        for stage in self.environment.models.get('models', {}).get('stages', []):
            content += f"""
- **{stage['name']}**:
  - ãƒ‘ã‚¹: `{stage['model_path']}`
  - GPUé…ç½®: {stage['gpu_ids']}
  - ä¸¦åˆ—åº¦: {stage['tensor_parallel_size']}
  - å®Ÿæ¸¬ã‚³ã‚¹ãƒˆ: {stage['relative_cost']}x
  - å®Ÿæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {stage['base_latency_ms']}ms
"""
        
        content += f"""
### 1.5 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š

#### è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®Œå…¨ç‰ˆä½¿ç”¨ï¼‰
"""
        
        for dataset in self.environment.datasets.get('datasets', {}).values():
            if isinstance(dataset, dict) and 'max_samples' in dataset:
                content += f"- **{dataset.get('path', 'Unknown')}**: {dataset['max_samples']:,} ã‚µãƒ³ãƒ—ãƒ«\n"
        
        content += f"""
### 1.6 å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **Lambdaå€¤**: {self.environment.parameters['lambda_values']}
- **è©¦è¡Œå›æ•°**: {self.environment.parameters['num_seeds']}
- **ä¿¡é ¼æ°´æº–**: {self.environment.parameters['confidence_level']}

---

"""
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(content)
    
    def record_results(self, main_results: Dict, ablation_studies: Dict = None, 
                      statistical_analysis: Dict = None, performance_metrics: Dict = None,
                      errors_warnings: List[str] = None, raw_data: Dict = None):
        """
        å®Ÿé¨“çµæœã®è©³ç´°è¨˜éŒ²
        
        Args:
            main_results: ãƒ¡ã‚¤ãƒ³ã®å®Ÿé¨“çµæœ
            ablation_studies: ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶çµæœ
            statistical_analysis: çµ±è¨ˆåˆ†æçµæœ
            performance_metrics: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šçµæœ
            errors_warnings: ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Šã®ãƒªã‚¹ãƒˆ
        """
        print("ğŸ“Š å®Ÿé¨“çµæœã‚’è¨˜éŒ²ä¸­...")
        
        self.results = ExperimentResults(
            timestamp=datetime.datetime.now().isoformat(),
            experiment_id=self.experiment_id,
            main_results=main_results or {},
            ablation_studies=ablation_studies or {},
            statistical_analysis=statistical_analysis or {},
            performance_metrics=performance_metrics or {},
            errors_warnings=errors_warnings or []
        )
        
        self._write_results_section()
        print("âœ… å®Ÿé¨“çµæœè¨˜éŒ²å®Œäº†")
    
    def _write_results_section(self):
        """çµæœæƒ…å ±ã‚’Markdownã«è¨˜éŒ²"""
        if not self.results:
            return
            
        content = f"""
## 2. å®Ÿé¨“çµæœ

### 2.1 ãƒ¡ã‚¤ãƒ³çµæœ

#### å…¨ä½“æ€§èƒ½ã‚µãƒãƒªãƒ¼
"""
        
        # ãƒ¡ã‚¤ãƒ³çµæœã®è¨˜éŒ²
        if 'summary' in self.results.main_results:
            summary = self.results.main_results['summary']
            content += f"""
- **å¹³å‡é«˜é€ŸåŒ–**: {summary.get('avg_speedup', 'N/A')}å€
- **å“è³ªä¿æŒç‡**: {summary.get('quality_retention', 'N/A')}%
- **ç·å‡¦ç†ã‚µãƒ³ãƒ—ãƒ«æ•°**: {summary.get('total_samples', 'N/A'):,}
- **å®Ÿé¨“å®Œäº†æ™‚åˆ»**: {self.results.timestamp}
"""
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®è©³ç´°è¨˜éŒ²
        if 'raw_data_summary' in self.results.main_results:
            raw_summary = self.results.main_results['raw_data_summary']
            content += f"""

#### ç”Ÿãƒ‡ãƒ¼ã‚¿è©³ç´°ã‚µãƒãƒªãƒ¼
- **è¨˜éŒ²ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ç·æ•°**: {raw_summary.get('total_samples_recorded', 'N/A'):,}
- **å¹³å‡å‡¦ç†æ™‚é–“/ã‚µãƒ³ãƒ—ãƒ«**: {raw_summary.get('avg_time_per_sample', 'N/A'):.3f}ç§’
- **æ®µéšåˆ¥ä½¿ç”¨åˆ†å¸ƒ**:
"""
            if 'stage_usage_distribution' in raw_summary:
                for stage, count in raw_summary['stage_usage_distribution'].items():
                    percentage = (count / raw_summary.get('total_samples_recorded', 1)) * 100
                    content += f"  - Stage {stage}: {count:,}å› ({percentage:.1f}%)\n"
            
            content += f"""
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è©³ç´°**:
  - æœ€å°: {raw_summary.get('latency_min', 'N/A'):.2f}ms
  - æœ€å¤§: {raw_summary.get('latency_max', 'N/A'):.2f}ms  
  - ä¸­å¤®å€¤: {raw_summary.get('latency_median', 'N/A'):.2f}ms
  - æ¨™æº–åå·®: {raw_summary.get('latency_std', 'N/A'):.2f}ms
"""
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµæœ
        if 'by_dataset' in self.results.main_results:
            content += "\n#### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµæœ\n\n"
            content += "| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚µãƒ³ãƒ—ãƒ«æ•° | ç²¾åº¦ | é«˜é€ŸåŒ– | å“è³ªä¿æŒ |\n"
            content += "|-------------|-----------|------|--------|----------|\n"
            
            for dataset, metrics in self.results.main_results['by_dataset'].items():
                content += f"| {dataset} | {metrics.get('samples', 'N/A'):,} | {metrics.get('accuracy', 'N/A'):.3f} | {metrics.get('speedup', 'N/A'):.2f}x | {metrics.get('quality', 'N/A'):.1f}% |\n"
        
        # Lambdaå€¤åˆ¥çµæœ
        if 'by_lambda' in self.results.main_results:
            content += "\n#### Lambdaå€¤åˆ¥æ€§èƒ½\n\n"
            content += "| Lambda | é«˜é€ŸåŒ– | å“è³ª | æ—©æœŸåœæ­¢ç‡ |\n"
            content += "|--------|--------|------|------------|\n"
            
            for lambda_val, metrics in self.results.main_results['by_lambda'].items():
                content += f"| {lambda_val} | {metrics.get('speedup', 'N/A'):.2f}x | {metrics.get('quality', 'N/A'):.3f} | {metrics.get('early_stop_rate', 'N/A'):.1f}% |\n"
        
        # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶
        if self.results.ablation_studies:
            content += "\n### 2.2 ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ï¼ˆè¦ç´ åˆ†æï¼‰\n\n"
            
            for component, results in self.results.ablation_studies.items():
                content += f"#### {component}ã®å½±éŸ¿\n\n"
                if isinstance(results, dict):
                    for metric, value in results.items():
                        content += f"- **{metric}**: {value}\n"
                content += "\n"
        
        # çµ±è¨ˆåˆ†æ
        if self.results.statistical_analysis:
            content += "\n### 2.3 çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š\n\n"
            
            if 'significance_tests' in self.results.statistical_analysis:
                content += "#### æœ‰æ„æ€§æ¤œå®šçµæœ\n\n"
                for test_name, result in self.results.statistical_analysis['significance_tests'].items():
                    content += f"- **{test_name}**: på€¤ = {result.get('p_value', 'N/A')}, æœ‰æ„ = {result.get('significant', 'N/A')}\n"
            
            if 'confidence_intervals' in self.results.statistical_analysis:
                content += "\n#### ä¿¡é ¼åŒºé–“\n\n"
                for metric, ci in self.results.statistical_analysis['confidence_intervals'].items():
                    content += f"- **{metric}**: [{ci.get('lower', 'N/A'):.3f}, {ci.get('upper', 'N/A'):.3f}] (95%ä¿¡é ¼åŒºé–“)\n"
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        if self.results.performance_metrics:
            content += "\n### 2.4 è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š\n\n"
            
            if 'latency' in self.results.performance_metrics:
                content += "#### ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š\n\n"
                latency = self.results.performance_metrics['latency']
                content += f"- **å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: {latency.get('mean', 'N/A'):.2f}ms\n"
                content += f"- **æ¨™æº–åå·®**: {latency.get('std', 'N/A'):.2f}ms\n"
                content += f"- **P95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: {latency.get('p95', 'N/A'):.2f}ms\n"
            
            if 'throughput' in self.results.performance_metrics:
                content += "\n#### ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š\n\n"
                throughput = self.results.performance_metrics['throughput']
                content += f"- **å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {throughput.get('mean', 'N/A'):.2f} tokens/sec\n"
                content += f"- **ãƒ”ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {throughput.get('peak', 'N/A'):.2f} tokens/sec\n"
        
        # ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Š
        if self.results.errors_warnings:
            content += "\n### 2.5 ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Š\n\n"
            for i, error in enumerate(self.results.errors_warnings, 1):
                content += f"{i}. {error}\n"
        
        content += "\n---\n\n"
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(content)
    
    def add_section(self, title: str, content: str):
        """ã‚«ã‚¹ã‚¿ãƒ ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ """
        section_content = f"""
## {title}

{content}

---

"""
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(section_content)
    
    def save_data(self):
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’JSONã§ä¿å­˜"""
        data = {
            "environment": asdict(self.environment) if self.environment else None,
            "results": asdict(self.results) if self.results else None
        }
        
        with open(self.json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    def finalize(self):
        """ãƒ­ã‚°ã®æœ€çµ‚åŒ–"""
        # æœ€çµ‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ 
        final_content = f"""
## 3. è«–æ–‡åŸ·ç­†ç”¨æƒ…å ±

### 3.1 å†ç¾æ€§ã®ãŸã‚ã®é‡è¦æƒ…å ±

æœ¬å®Ÿé¨“ã¯ä»¥ä¸‹ã®ç’°å¢ƒã§å®Ÿè¡Œã•ã‚Œã¾ã—ãŸï¼š

- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢**: {len(self.environment.hardware['gpu']) if self.environment else 'N/A'}x GPUæ§‹æˆ
- **ãƒ¢ãƒ‡ãƒ«**: Qwen2.5 4æ®µéšéšå±¤ï¼ˆ7Bâ†’14Bâ†’32Bâ†’72Bï¼‰
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: å®Œå…¨ç‰ˆï¼ˆç·è¨ˆ{sum(d.get('max_samples', 0) for d in self.environment.datasets.get('datasets', {}).values() if isinstance(d, dict)) if self.environment else 'N/A':,}ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- **å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: Lambdaå€¤6ç‚¹ã€ã‚·ãƒ¼ãƒ‰5å›

### 3.2 ä¸»è¦ãªçŸ¥è¦‹

1. **åŠ¹ç‡æ€§**: å¹³å‡{self.results.main_results.get('summary', {}).get('avg_speedup', 'N/A')}å€ã®é«˜é€ŸåŒ–ã‚’é”æˆ
2. **å“è³ª**: {self.results.main_results.get('summary', {}).get('quality_retention', 'N/A')}%ã®å“è³ªä¿æŒã‚’å®Ÿç¾
3. **ç†è«–çš„ä¿è¨¼**: æœ€é©åœæ­¢ç†è«–ã«ã‚ˆã‚‹æ•°å­¦çš„è£ä»˜ã‘

### 3.3 ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±

- **åŒ…æ‹¬ãƒ­ã‚°**: `{self.log_file.name}`
- **æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿**: `{self.json_file.name}`
- **å®Ÿé¨“ID**: `{self.experiment_id}`

---

**å®Ÿé¨“å®Œäº†**: {datetime.datetime.now().isoformat()}

æœ¬ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¯è«–æ–‡åŸ·ç­†æ™‚ã®å‚è€ƒè³‡æ–™ã¨ã—ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
ã™ã¹ã¦ã®å®Ÿé¨“è©³ç´°ãŒè¨˜éŒ²ã•ã‚Œã¦ãŠã‚Šã€å®Œå…¨ãªå†ç¾ãŒå¯èƒ½ã§ã™ã€‚
"""
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(final_content)
        
        # JSONãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.save_data()
        
        print(f"ğŸ“ åŒ…æ‹¬ãƒ­ã‚°ä½œæˆå®Œäº†:")
        print(f"   ğŸ“„ Markdown: {self.log_file}")
        print(f"   ğŸ’¾ JSON Data: {self.json_file}")
        print(f"   ğŸ†” Experiment ID: {self.experiment_id}")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    logger = ComprehensiveLogger("full_evaluation_test")
    
    # ç’°å¢ƒã‚­ãƒ£ãƒ—ãƒãƒ£
    logger.capture_environment()
    
    # ã‚µãƒ³ãƒ—ãƒ«çµæœè¨˜éŒ²
    sample_results = {
        "summary": {
            "avg_speedup": 3.6,
            "quality_retention": 91.2,
            "total_samples": 16342
        },
        "by_dataset": {
            "MMLU": {"samples": 14042, "accuracy": 0.852, "speedup": 3.2, "quality": 90.5},
            "GSM8K": {"samples": 1319, "accuracy": 0.743, "speedup": 4.1, "quality": 92.8}
        }
    }
    
    ablation_results = {
        "quality_predictor": {
            "with_predictor": 3.6,
            "without_predictor": 1.8,
            "improvement": "2.0x"
        }
    }
    
    logger.record_results(sample_results, ablation_results)
    logger.finalize()