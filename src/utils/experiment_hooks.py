#!/usr/bin/env python3
"""
å®Ÿé¨“ãƒ•ãƒƒã‚¯ï¼šæ—¢å­˜ã®å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«åŒ…æ‹¬ãƒ­ã‚°ã‚’çµ±åˆ

æ—¢å­˜ã®å®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã‚’æœ€å°é™ã®å¤‰æ›´ã§åŒ…æ‹¬ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œ
"""

import functools
import time
import traceback
from typing import Callable, Any, Dict, Optional
from pathlib import Path

try:
    from .comprehensive_logger import ComprehensiveLogger
except ImportError:
    from comprehensive_logger import ComprehensiveLogger

class ExperimentTracker:
    """
    æ—¢å­˜å®Ÿé¨“ã¸ã®åŒ…æ‹¬ãƒ­ã‚°çµ±åˆ
    
    ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§
    æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã«æœ€å°é™ã®å¤‰æ›´ã§çµ±åˆ
    """
    
    _current_logger: Optional[ComprehensiveLogger] = None
    _experiment_data: Dict[str, Any] = {}
    
    @classmethod
    def initialize(cls, experiment_name: str) -> ComprehensiveLogger:
        """å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–"""
        cls._current_logger = ComprehensiveLogger(experiment_name)
        cls._experiment_data = {
            "start_time": time.time(),
            "results": {},
            "errors": [],
            "performance": {}
        }
        
        # ç’°å¢ƒã‚­ãƒ£ãƒ—ãƒãƒ£
        cls._current_logger.capture_environment()
        
        return cls._current_logger
    
    @classmethod
    def get_logger(cls) -> Optional[ComprehensiveLogger]:
        """ç¾åœ¨ã®ãƒ­ã‚¬ãƒ¼å–å¾—"""
        return cls._current_logger
    
    @classmethod
    def record_result(cls, section: str, data: Any):
        """çµæœè¨˜éŒ²"""
        if cls._current_logger:
            cls._experiment_data["results"][section] = data
    
    @classmethod
    def record_error(cls, error: str):
        """ã‚¨ãƒ©ãƒ¼è¨˜éŒ²"""
        cls._experiment_data["errors"].append(error)
    
    @classmethod
    def record_performance(cls, metric: str, value: Any):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²"""
        cls._experiment_data["performance"][metric] = value
    
    @classmethod
    def finalize(cls):
        """å®Ÿé¨“å®Œäº†å‡¦ç†"""
        if cls._current_logger:
            # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
            total_time = time.time() - cls._experiment_data["start_time"]
            cls._experiment_data["performance"]["total_runtime_seconds"] = total_time
            
            # çµæœè¨˜éŒ²
            cls._current_logger.record_results(
                main_results=cls._experiment_data["results"],
                performance_metrics=cls._experiment_data["performance"],
                errors_warnings=cls._experiment_data["errors"]
            )
            
            # ãƒ­ã‚°å®Œæˆ
            cls._current_logger.finalize()
            
            return cls._current_logger.experiment_id
        
        return None

def track_experiment(experiment_name: str):
    """
    å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    
    Usage:
        @track_experiment("my_experiment")
        def run_my_experiment():
            # å®Ÿé¨“ã‚³ãƒ¼ãƒ‰
            pass
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # å®Ÿé¨“é–‹å§‹
            logger = ExperimentTracker.initialize(experiment_name)
            print(f"ğŸš€ å®Ÿé¨“é–‹å§‹: {experiment_name}")
            print(f"ğŸ“ ãƒ­ã‚°ID: {logger.experiment_id}")
            
            try:
                # å®Ÿé¨“å®Ÿè¡Œ
                result = func(*args, **kwargs)
                
                # æ­£å¸¸å®Œäº†
                print("âœ… å®Ÿé¨“æ­£å¸¸å®Œäº†")
                experiment_id = ExperimentTracker.finalize()
                print(f"ğŸ“„ ãƒ­ã‚°ä¿å­˜: {logger.log_file}")
                
                return result
                
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼å‡¦ç†
                error_msg = f"å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {str(e)}"
                print(f"âŒ {error_msg}")
                
                ExperimentTracker.record_error(error_msg)
                ExperimentTracker.record_error(traceback.format_exc())
                ExperimentTracker.finalize()
                
                raise
        
        return wrapper
    return decorator

def log_section(section_name: str):
    """
    ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    
    Usage:
        @log_section("dataset_evaluation")
        def evaluate_datasets():
            # è©•ä¾¡ã‚³ãƒ¼ãƒ‰
            return results
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            print(f"ğŸ“Š {section_name} é–‹å§‹...")
            start_time = time.time()
            
            try:
                result = func(*args, **kwargs)
                
                # å®Ÿè¡Œæ™‚é–“è¨˜éŒ²
                duration = time.time() - start_time
                ExperimentTracker.record_performance(f"{section_name}_duration", duration)
                
                # çµæœè¨˜éŒ²
                if result is not None:
                    ExperimentTracker.record_result(section_name, result)
                
                print(f"âœ… {section_name} å®Œäº† ({duration:.1f}ç§’)")
                return result
                
            except Exception as e:
                error_msg = f"{section_name}ã§ã‚¨ãƒ©ãƒ¼: {str(e)}"
                ExperimentTracker.record_error(error_msg)
                print(f"âŒ {error_msg}")
                raise
        
        return wrapper
    return decorator

class ExperimentContext:
    """
    å®Ÿé¨“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    
    Usage:
        with ExperimentContext("my_experiment") as ctx:
            # å®Ÿé¨“ã‚³ãƒ¼ãƒ‰
            ctx.log_result("accuracy", 0.95)
    """
    
    def __init__(self, experiment_name: str):
        self.experiment_name = experiment_name
        self.logger: Optional[ComprehensiveLogger] = None
    
    def __enter__(self):
        self.logger = ExperimentTracker.initialize(self.experiment_name)
        print(f"ğŸš€ å®Ÿé¨“é–‹å§‹: {self.experiment_name}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ
            error_msg = f"å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {str(exc_val)}"
            ExperimentTracker.record_error(error_msg)
            print(f"âŒ {error_msg}")
        
        # å®Œäº†å‡¦ç†
        experiment_id = ExperimentTracker.finalize()
        if self.logger:
            print(f"ğŸ“„ ãƒ­ã‚°ä¿å­˜: {self.logger.log_file}")
        
        return False  # ä¾‹å¤–ã‚’å†ç™ºç”Ÿ
    
    def log_result(self, key: str, value: Any):
        """çµæœè¨˜éŒ²"""
        ExperimentTracker.record_result(key, value)
    
    def log_performance(self, metric: str, value: Any):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²"""
        ExperimentTracker.record_performance(metric, value)
    
    def log_section(self, section: str, data: Dict):
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²"""
        ExperimentTracker.record_result(section, data)

# ä¾¿åˆ©é–¢æ•°
def quick_log(key: str, value: Any):
    """ã‚¯ã‚¤ãƒƒã‚¯çµæœè¨˜éŒ²"""
    ExperimentTracker.record_result(key, value)

def log_metric(metric: str, value: Any):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
    ExperimentTracker.record_performance(metric, value)

def log_error(error: str):
    """ã‚¨ãƒ©ãƒ¼è¨˜éŒ²"""
    ExperimentTracker.record_error(error)

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    import random
    import numpy as np
    
    # ä¾‹1: ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ä½¿ç”¨
    @track_experiment("decorator_test")
    def test_with_decorator():
        
        @log_section("data_preparation")
        def prepare_data():
            time.sleep(1)  # å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            return {"samples": 1000, "features": 512}
        
        @log_section("model_training")  
        def train_model():
            time.sleep(2)
            return {"accuracy": 0.95, "loss": 0.12}
        
        @log_section("evaluation")
        def evaluate():
            time.sleep(1)
            return {"test_accuracy": 0.93, "f1_score": 0.91}
        
        # å®Ÿè¡Œ
        data_info = prepare_data()
        training_results = train_model() 
        eval_results = evaluate()
        
        # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        log_metric("total_parameters", 1000000)
        log_metric("training_time", 3600)
        
        return {
            "final_accuracy": eval_results["test_accuracy"],
            "status": "success"
        }
    
    # ä¾‹2: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½¿ç”¨
    def test_with_context():
        with ExperimentContext("context_test") as ctx:
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            ctx.log_section("preparation", {
                "dataset": "MMLU",
                "samples": 14042
            })
            
            # è©•ä¾¡å®Ÿè¡Œ
            results = {
                "accuracy": 0.852,
                "speedup": 3.6,
                "quality": 91.2
            }
            ctx.log_result("main_results", results)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²
            ctx.log_performance("latency_ms", 847.3)
            ctx.log_performance("throughput_tps", 47.2)
    
    print("ğŸ§ª åŒ…æ‹¬ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹\n")
    
    # ãƒ†ã‚¹ãƒˆ1
    print("=" * 50)
    print("ãƒ†ã‚¹ãƒˆ1: ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ä½¿ç”¨")
    print("=" * 50)
    test_with_decorator()
    
    print("\n" + "=" * 50)
    print("ãƒ†ã‚¹ãƒˆ2: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½¿ç”¨")
    print("=" * 50)
    test_with_context()
    
    print("\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")